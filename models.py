import torchaudio
import torch.nn as nn

class LatentExtractor(nn.Module):
    def __init__(self):
        super().__init__()
        self.hubert = torchaudio.pipelines.HUBERT_BASE.get_model()
        self.hubert = self.hubert.to('cuda')
        self.hubert.eval()
        for p in self.hubert.parameters():
            p.requires_grad = False

        self.latent_dim = 768

    def forward(self, waveform):
        with torch.no_grad():
            features, _ = self.hubert.extract_features(waveform)
            hubert_feats = features[-1]  # (B, T, 768)

        hubert_latent = torch.mean(hubert_feats, dim=1)  # (B, 768)
        return hubert_latent

class WaveformToMelLSTM(nn.Module):
    def __init__(self,
                 n_mels=80,
                 sample_rate=16000,
                 n_fft=1024,
                 hop_length=256,
                 hidden_size=256,
                 num_layers=2,
                 cond_dim=768):
        super(WaveformToMelLSTM, self).__init__()

        self.n_fft = n_fft
        self.hop_length = hop_length
        self.n_mels = n_mels
        self.sample_rate = sample_rate
        self.hidden_size = hidden_size
        self.num_layers = num_layers

        self.frame_proj = nn.Conv1d(1, hidden_size, kernel_size=hop_length, stride=hop_length)

        # Project conditioning to hidden_size for h0
        self.cond_proj = nn.Linear(cond_dim, hidden_size * 2)  # *2 because of bidirectional

        self.lstm = nn.LSTM(input_size=hidden_size,
                            hidden_size=hidden_size,
                            num_layers=num_layers,
                            batch_first=True,
                            bidirectional=True)

        self.out_proj = nn.Linear(hidden_size * 2, n_mels)

    def forward(self, waveform, voice_embedding):
        """
        waveform: [B, 1, T]
        voice_embedding: [B, 768]
        returns: mel [B, n_mels, T']
        """
        B, C, T = waveform.shape
        if C != 1:
            raise ValueError("Waveform must be mono, shape [B, 1, T]")

        x = self.frame_proj(waveform)  # [B, hidden, T']
        x = x.transpose(1, 2)          # [B, T', hidden]

        # Project voice embedding to initial hidden state
        h0 = self.cond_proj(voice_embedding)  # [B, hidden_size * 2]
        h0 = h0.view(B, 2, self.hidden_size).transpose(0, 1)  # [2, B, hidden_size]
        h0 = h0.repeat(self.num_layers, 1, 1)  # [2 * num_layers, B, hidden_size]
        c0 = torch.zeros_like(h0)

        x, _ = self.lstm(x, (h0, c0))  # [B, T', hidden*2]
        mel = self.out_proj(x).transpose(1, 2)  # [B, n_mels, T']
        return mel

import torch
import torch.nn as nn

class WaveformToWaveformModel(nn.Module):
    def __init__(self, latent_extractor, waveform_to_mel, hifigan, trim_size = 64000):
        """
        Args:
            latent_extractor (nn.Module): Module that extracts latent from waveform_color
            waveform_to_mel (nn.Module): LSTM-based mel generator (accepts waveform + latent)
            hifigan (nn.Module): HiFi-GAN vocoder that takes mel [B, 80, T] and outputs waveform
        """
        super(WaveformToWaveformModel, self).__init__()
        self.latent_extractor = latent_extractor
        self.waveform_to_mel = waveform_to_mel
        self.hifigan = hifigan
        self.trim_size = trim_size

    def forward(self, waveform, waveform_color=None):
        """
        Args:
            waveform (Tensor): [B, 1, T] distorted waveform input
            waveform_color (Tensor): [B, 1, T_color], optional clean/reference waveform

        Returns:
            generated_waveform (Tensor): [B, 1, T'] waveform generated by hifigan
            mel (Tensor): [B, 80, T'] intermediate mel spectrogram
            latent (Tensor): [B, latent_dim] extracted speaker/voice color vector
        """
        if waveform_color is not None:
            latent = self.latent_extractor(waveform_color.squeeze())  # [B, latent_dim]
        else:
            # Fall back to zero vector if no color provided
            B = waveform.size(0)
            latent_dim = self.waveform_to_mel.cond_proj.in_features
            latent = torch.zeros(B, latent_dim, device=waveform.device)

        mel = self.waveform_to_mel(waveform, latent)  # [B, 80, T']
        generated_waveform = self.hifigan.mods.generator(mel)
        generated_waveform = generated_waveform[..., :self.trim_size]

        return generated_waveform, mel, latent

